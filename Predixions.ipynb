{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17481e8d-93b4-45d3-b3c8-ef81b7744c31",
   "metadata": {},
   "source": [
    "# Install Spark, PySpark and Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a513c-2e0c-4754-ae7f-5a74d49140e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Python Notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c2b83-6c92-47a9-93dd-cb72ed24e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install apache-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13003dd-ac5f-4337-aa16-634a33552ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Completed installing apache-spark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd91fb72-3dc8-47da-8a11-1cbb99d37387",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install openjdk@11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6e1f8-eb81-4278-aaa7-6a0ee605a01b",
   "metadata": {},
   "source": [
    " The above Java installation doesn't work immediately. If you do a \"re-install\" it tells which commands need to be run to fix the path variable etc.\n",
    "\n",
    "(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') >> /Users/ganapathychidambaram/.zprofile\n",
    "\n",
    "   \n",
    "eval \"$(/opt/homebrew/bin/brew shellenv)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425c723-0643-4240-99fd-2b69e3de667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which java\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38c1c6-21f0-4160-a73b-b86c276fa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export JAVA_HOME=/usr\n",
    "!export PATH=$JAVA_HOME/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18229829-990e-4c42-98ca-334af4589231",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd784401-e13d-4d25-b350-48345abe1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export SPARK_HOME=/opt/homebrew/Cellar/apache-spark/3.3.0/libexec\n",
    "!export PATH=/opt/homebrew/Cellar/apache-spark/3.3.0/bin:$PATH\n",
    "\n",
    "# looks the case of the JAVA_HOME variable made all the difference    \n",
    "\n",
    "!export PYSPARK_SUBMIT_ARGS=\"--master local[3] pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde423b-4dad-4a69-9c95-416684911096",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyspark\n",
    "!pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088f31a-be45-4491-91be-032931a9221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36414768-d885-4265-b091-44dd1e7fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b5fdf-3532-486f-9e9e-e4fe01bfef1b",
   "metadata": {},
   "source": [
    "# Start Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d759f80-fd43-449b-bce8-276a0b083bfa",
   "metadata": {},
   "source": [
    "### Load the applications data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02160d43-3a56-451d-8430-dae67fd3051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"/Users/ganapathychidambaram/Desktop/predixions/apps.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1c07e-60ba-4b73-91fb-42eac9e6dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ebe697-a1de-4dc3-b9b9-cb756d09b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2078e2-c53b-4605-ae60-93c44108b464",
   "metadata": {},
   "source": [
    "pre-processing = Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9a6a5-38cb-4c46-8136-158ea002ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_df = applications_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4a8bc-d386-403b-b91e-3a234f4e16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3ad1d-127b-4a97-bc92-acefe92d0ff1",
   "metadata": {},
   "source": [
    "### Load the users data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3a08e-c860-431a-a9e4-59136cf03a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"/Users/ganapathychidambaram/Desktop/predixions/users.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95e9cc-c443-42cc-b07e-6018d312d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf009d6-109b-406c-8e00-efd11a171755",
   "metadata": {},
   "source": [
    "##### Data Analysis (EDA) on the users data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549cff6-9a46-486f-97c9-bb2ca281e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = users_df.select('DegreeType').distinct()\n",
    "unique_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbb354-cd9c-41e3-9b69-50288c8bdb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = users_df.select('Major').distinct()\n",
    "print(unique_values.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e554b-674a-4999-9ffb-f72557c3a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "users_df.filter(col('Major') == \"Not Applicable\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f43cf-62f0-40d1-8e11-f26364e7031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.filter(col('Major') == \"None\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b647b06-175e-429c-9095-9dfa2b224f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.filter(col('Major').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da707652-6988-4dcc-ae13-54f4d4c92b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.filter(col('Major') == \"null\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26962a1c-e2d5-4b7d-9803-fd5424cef182",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.filter(col('CurrentlyEmployed').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ba9b1-b725-41c9-8c0c-a6885248da14",
   "metadata": {},
   "source": [
    "Pre-processing - convert None to Not Applicable; Category Indexing, Normalizing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c554fea-a393-487a-9379-1b65d7f2e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values with 'No' in a specific column (e.g., 'ColumnName')\n",
    "\n",
    "users_df = users_df.na.fill('No', ['CurrentlyEmployed', 'ManagedOthers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6e006-c17c-4106-90b9-d3a8476c2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = users_df.select('CurrentlyEmployed').distinct()\n",
    "unique_values.show()\n",
    "\n",
    "unique_values = users_df.select('ManagedOthers').distinct()\n",
    "unique_values.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474505e-612e-41d7-96ff-2d28a6d786e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = users_df.withColumn('Major', when(col('Major') == 'None', 'Not Applicable').otherwise(col('Major')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea83be-e0ee-4e39-aeb9-af77b2a40504",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93985d9a-fc84-4b76-a449-ff6ea51a9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ca3ff-996b-4715-a201-9a1cc3888d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='DegreeType', outputCol='DegreeType_encoded', handleInvalid='skip')\n",
    "users_df_encoded = string_indexer.fit(users_df).transform(users_df)\n",
    "\n",
    "users_df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb28231-d058-436c-bfba-f8b3f278a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numerically encode these columns ['City', 'State', 'Country', 'CurrentlyEmployed', 'ManagedOthers']\n",
    "categorical_columns = ['City', 'State', 'Country', 'CurrentlyEmployed', 'ManagedOthers']\n",
    "\n",
    "for cat_col in categorical_columns:\n",
    "    encoded_column = cat_col + \"_encoded\"\n",
    "    string_indexer = StringIndexer(inputCol=cat_col, outputCol=encoded_column, handleInvalid='skip')\n",
    "    users_df_encoded = string_indexer.fit(users_df_encoded).transform(users_df_encoded)\n",
    "\n",
    "# Show the resulting DataFrame with the encoded column\n",
    "users_df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352282d-48e2-4d22-bfd1-7788290d92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['City', 'State', 'Country', 'CurrentlyEmployed', 'ManagedOthers', 'DegreeType']\n",
    "users_df_encoded = users_df_encoded.drop(*columns_to_drop)\n",
    "users_df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5a67e-2160-4324-a12f-c4c5d5d55ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "# Calculate the frequency of each unique value in the 'Major' column\n",
    "major_frequencies = users_df_encoded.groupBy('Major').agg(count(lit(1)).alias('major_frequency'))\n",
    "\n",
    "# Join the frequency information back to the original DataFrame\n",
    "users_df_encoded = users_df_encoded.join(major_frequencies, on='Major', how='left')\n",
    "\n",
    "# Replace null values in the 'Frequency' column with 0\n",
    "users_df_encoded = users_df_encoded.na.fill(0, ['major_frequency'])\n",
    "\n",
    "users_df_encoded = users_df_encoded.drop('Major')\n",
    "users_df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d06806-70a3-40ca-8954-d0b655daf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_encoded = users_df_encoded.withColumnRenamed(\"ZipCode\",\"u_ZipCode\").withColumnRenamed(\"City_encoded\",\"u_City_encoded\").withColumnRenamed(\"State_encoded\", \"u_State_encoded\")\n",
    "users_df_encoded = users_df_encoded.withColumnRenamed(\"Country_encoded\", \"u_Country_encoded\")\n",
    "users_df_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894dbf9-6026-43fb-8802-76d1ef5b2c1d",
   "metadata": {},
   "source": [
    "### Load User History file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eff8a9-4a04-4821-ae61-57fd4e2bbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"/Users/ganapathychidambaram/Desktop/predixions/user_history.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502c090-6fff-479b-b84e-28a1f95ec128",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916182b-0a81-4e6c-857f-699799d7df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = user_history_df.select('JobTitle').distinct()\n",
    "unique_values.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d9f36-a770-40c0-95f8-ca690ce32f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49bac3-de5b-4c7f-a4ee-be2df9c04ab7",
   "metadata": {},
   "source": [
    "#### Do Category encoding on the Job Titles - Encode them by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e078de-dde1-4c29-9a2c-bb7cc6723ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_frequencies = user_history_df.groupBy('JobTitle').agg(count(lit(1)).alias('job_title_freq'))\n",
    "job_title_frequencies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f900570-b089-42c7-99dc-af27feca3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the frequency information back to the original DataFrame\n",
    "user_history_df = user_history_df.join(job_title_frequencies, on='JobTitle', how='left')\n",
    "\n",
    "# Replace null values in the 'Frequency' column with 0\n",
    "user_history_df = user_history_df.na.fill(0, ['job_title_freq'])\n",
    "\n",
    "# Drop the original 'JobTitle' column\n",
    "user_history_df = user_history_df.drop('JobTitle')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "user_history_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acdcc6-575b-4067-972d-f372fac0a9f5",
   "metadata": {},
   "source": [
    "#### Flatten the data frame, such that there is one user per row with multiple job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e3e4b-4c47-4b49-acb4-1bb23e43c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, first\n",
    "\n",
    "compressed_user_hist_df = user_history_df.groupBy('UserID').agg(\n",
    "    collect_list('Sequence').alias('Sequence'),\n",
    "    collect_list('job_title_freq').alias('job_title_freq'),\n",
    "    first('WindowID').alias('WindowID'),\n",
    "    first('Split').alias('Split')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caef775-4d31-4d60-94c7-5dc2a3a297f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_user_hist_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e6631-0e15-4255-a14c-47e51e1cb5eb",
   "metadata": {},
   "source": [
    "### Load jobs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bd87d-1d15-4602-83f7-addca95d5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"/Users/ganapathychidambaram/Desktop/predixions/jobs.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb71cd5-4fde-444a-9652-d994fa0e66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77267b09-8fe5-40f2-acc7-d576a84c8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in 'Title' column with an empty string\n",
    "jobs_df = jobs_df.withColumn(\"Title\", when(jobs_df[\"Title\"].isNull(), \"\").otherwise(jobs_df[\"Title\"]))\n",
    "\n",
    "# Replace null values in 'Description' and 'Requirements' columns\n",
    "jobs_df = jobs_df.fillna({\"Description\": \"<p>No description available</p>\", \"Requirements\": \"<p>No requirements available</p>\"})\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "jobs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4a4ce-3b9c-4974-99a3-f99257b53e18",
   "metadata": {},
   "source": [
    "Since Description and Requirments are long texts, do TF-IDF encoding to represent them. TBD: Check if this good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cfc03-9b27-4508-a27c-1174bb0ad9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "# Concatenate the text from different columns into a single column\n",
    "jobs_df = jobs_df.withColumn(\"combined_text\", concat(col(\"Description\"), lit(\" \"), col(\"Requirements\")))\n",
    "\n",
    "# Tokenize the combined text\n",
    "tokenizer = Tokenizer(inputCol=\"combined_text\", outputCol=\"tokens\")\n",
    "jobs_df = tokenizer.transform(jobs_df)\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "jobs_df = stopwords_remover.transform(jobs_df)\n",
    "\n",
    "# Compute Term Frequencies\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"tf_features\")\n",
    "count_vectorizer_model = count_vectorizer.fit(jobs_df)\n",
    "jobs_df = count_vectorizer_model.transform(jobs_df)\n",
    "\n",
    "# Compute Inverse Document Frequencies\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(jobs_df)\n",
    "jobs_df = idf_model.transform(jobs_df)\n",
    "\n",
    "# Drop intermediate columns\n",
    "jobs_df = jobs_df.drop(\"combined_text\", \"tokens\", \"filtered_tokens\", \"tf_features\")\n",
    "\n",
    "# Drop the source columns\n",
    "jobs_df = jobs_df.drop(\"Title\", \"Description\", \"Requirements\")\n",
    "\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "jobs_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24cb87-548d-47f0-bc3a-ae6c801a8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in jobs_df.columns:\n",
    "    null_count = jobs_df.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"Column '{col_name}' has {null_count} null value(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f7ba5-7ace-4d85-81a4-60be221f10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null zip codes with a default zip code (e.g., '00000')\n",
    "jobs_df = jobs_df.withColumn('Zip5', coalesce('Zip5', lit('00000')))\n",
    "jobs_df = jobs_df.withColumn('EndDate', coalesce('EndDate', lit('1970-01-01')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee989e6-ced7-4dba-85e5-e89f0472e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerically encode these columns ['City', 'State', 'Country', 'CurrentlyEmployed', 'ManagedOthers']\n",
    "categorical_columns = ['City', 'State', 'Country']\n",
    "\n",
    "for cat_col in categorical_columns:\n",
    "    encoded_column = cat_col + \"_encoded\"\n",
    "    string_indexer = StringIndexer(inputCol=cat_col, outputCol=encoded_column, handleInvalid='skip')\n",
    "    jobs_df = string_indexer.fit(jobs_df).transform(jobs_df)\n",
    "\n",
    "columns_to_drop = ['City', 'State', 'Country']\n",
    "jobs_df = jobs_df.drop(*columns_to_drop)\n",
    "# Show the resulting DataFrame with the encoded column\n",
    "jobs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107fc5a-a482-41db-b1ab-64c9f7efeb4d",
   "metadata": {},
   "source": [
    "# Merge the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a518b3c-cc2a-40a1-a65a-1b72acc22508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join based on three columns\n",
    "# Alias the columns in the first DataFrame\n",
    "users_df_encoded = users_df_encoded.alias('users_df_encoded')\n",
    "df1_column1 = col('users_df_encoded.UserID')\n",
    "df1_column2 = col('users_df_encoded.WindowID')\n",
    "df1_column3 = col('users_df_encoded.Split')\n",
    "\n",
    "# Alias the columns in the second DataFrame\n",
    "compressed_user_hist_df = compressed_user_hist_df.alias('compressed_user_hist_df')\n",
    "df2_column1 = col('compressed_user_hist_df.UserID')\n",
    "df2_column2 = col('compressed_user_hist_df.WindowID')\n",
    "df2_column3 = col('compressed_user_hist_df.Split')\n",
    "\n",
    "\n",
    "\n",
    "merged_users_df_encoded = users_df_encoded.join(compressed_user_hist_df, on=[df1_column1 == df2_column1,\n",
    "                             df1_column2 == df2_column2,\n",
    "                             df1_column3 == df2_column3],\n",
    "                     how='left')\n",
    "\n",
    "# Drop the duplicate columns from the second DataFrame\n",
    "columns_to_drop = ['UserID', 'WindowID', 'Split'] \n",
    "for col_name in columns_to_drop:\n",
    "    merged_users_df_encoded = merged_users_df_encoded.drop(compressed_user_hist_df[col_name])\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "merged_users_df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11b9e7-20ef-4710-8cd4-1409dc361ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_users_df_encoded = merged_users_df_encoded.withColumn('TotalYearsExperience', when(df['TotalYearsExperience'].isNull(), lit(0)).otherwise(df['TotalYearsExperience']))\n",
    "from pyspark.sql.functions import coalesce, array\n",
    "\n",
    "merged_users_df_encoded = merged_users_df_encoded.fillna({'TotalYearsExperience': 0})\n",
    "# Impute nulls in the 'major_frequency' column with a default value (e.g., 0)\n",
    "merged_users_df_encoded = merged_users_df_encoded.fillna({'major_frequency': 0})\n",
    "\n",
    "\n",
    "# Replace None with an empty list for the list columns\n",
    "merged_users_df_encoded = merged_users_df_encoded.withColumn('Sequence', coalesce('Sequence', array()))\n",
    "merged_users_df_encoded = merged_users_df_encoded.withColumn('job_title_freq', coalesce('job_title_freq', array()))\n",
    "\n",
    "merged_users_df_encoded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90ac60-c7d1-4220-b0f6-f7c82fb11743",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in merged_users_df_encoded.columns:\n",
    "    null_count = merged_users_df_encoded.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"Column '{col_name}' has {null_count} null value(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89ba31-8b29-44e2-a620-3936f1124c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null zip codes with a default zip code (e.g., '00000')\n",
    "merged_users_df_encoded = merged_users_df_encoded.withColumn('u_ZipCode', coalesce('u_ZipCode', lit('00000')))\n",
    "merged_users_df_encoded = merged_users_df_encoded.withColumn('GraduationDate', coalesce('GraduationDate', lit('1970-01-01')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981128b-cb67-41e3-99b7-4330fe39f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in applications_df.columns:\n",
    "    null_count = applications_df.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"Column '{col_name}' has {null_count} null value(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36fee3-e29e-44b1-b44c-3f8608a03e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_df.printSchema()\n",
    "merged_users_df_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99f42e-58aa-4966-938b-42a67fc5c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_df = applications_df.alias('applications_df')\n",
    "df_left_column1 = col('applications_df.UserID')\n",
    "df_left_column2 = col('applications_df.WindowID')\n",
    "df_left_column3 = col('applications_df.Split')\n",
    "\n",
    "# Alias the columns in the second DataFrame\n",
    "merged_users_df_encoded = merged_users_df_encoded.alias('merged_users_df_encoded')\n",
    "df_right_column1 = col('merged_users_df_encoded.UserID')\n",
    "df_right_column2 = col('merged_users_df_encoded.WindowID')\n",
    "df_right_column3 = col('merged_users_df_encoded.Split')\n",
    "\n",
    "merged_app_df = applications_df.join(merged_users_df_encoded, on=[df_left_column1 == df_right_column1,\n",
    "                             df_left_column2 == df_right_column2,\n",
    "                             df_left_column3 == df_right_column3],\n",
    "                     how='left')\n",
    "\n",
    "\n",
    "# Drop the duplicate columns from the second DataFrame\n",
    "columns_to_drop = ['UserID', 'WindowID', 'Split'] \n",
    "for col_name in columns_to_drop:\n",
    "    merged_app_df = merged_app_df.drop(merged_users_df_encoded[col_name])\n",
    "  \n",
    "\n",
    "# Show the resulting DataFrame\n",
    "merged_app_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca8e4a-ed32-4bc4-807e-5c87e671916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_app_df = merged_app_df.alias('merged_app_df')\n",
    "df_left_column1 = col('merged_app_df.JobID')\n",
    "df_left_column2 = col('merged_app_df.WindowID')\n",
    "\n",
    "\n",
    "# Alias the columns in the second DataFrame\n",
    "jobs_df = jobs_df.alias('jobs_df')\n",
    "df_right_column1 = col('jobs_df.JobID')\n",
    "df_right_column2 = col('jobs_df.WindowID')\n",
    "\n",
    "\n",
    "total_df = merged_app_df.join(jobs_df, on=[df_left_column1 == df_right_column1, df_left_column2 == df_right_column2,],\n",
    "                              how='left')\n",
    "\n",
    "\n",
    "# Drop the duplicate columns from the second DataFrame\n",
    "columns_to_drop = ['JobID', 'WindowID'] \n",
    "for col_name in columns_to_drop:\n",
    "    total_df = total_df.drop(jobs_df[col_name])\n",
    "  \n",
    "\n",
    "# Show the resulting DataFrame\n",
    "total_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592ab57-35d7-4799-92c7-50190315d640",
   "metadata": {},
   "source": [
    "# Select the features and do Test vs Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac57a93-413e-4477-9163-1f4c3a8860ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You have omitted the application date as a feature. Check if it can be used a difference from the window start date\n",
    "features_list = ['UserID', 'WindowID', 'Split', 'JobID', 'u_ZipCode','GraduationDate', 'WorkHistoryCount', 'TotalYearsExperience', 'ManagedHowMany', 'DegreeType_encoded', 'u_City_encoded', 'u_State_encoded', 'u_Country_encoded', 'CurrentlyEmployed_encoded', 'ManagedOthers_encoded', 'major_frequency', 'Sequence', 'job_title_freq', 'Zip5', 'StartDate', 'EndDate', 'tfidf_features', 'City_encoded', 'State_encoded', 'Country_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2631ab2-0ea2-46c9-99de-2b81ac6c77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = total_df[features_list]\n",
    "\n",
    "#TODO: Casting to Integer seems too hacky - consider encoding this\n",
    "features_df = features_df.withColumn(\"UserID\", features_df[\"UserID\"].cast(\"integer\"))\n",
    "features_df = features_df.withColumn(\"JobID\", features_df[\"JobID\"].cast(\"integer\"))\n",
    "\n",
    "#features_df = features_df.withColumn(\"UserWindowID\", concat(col(\"UserID\"), lit(\"_\"), col(\"WindowID\")))\n",
    "train_features_df = features_df.filter(features_df['Split'] == 'Train')\n",
    "test_features_df = features_df.filter(features_df['Split'] == 'Test')\n",
    "\n",
    "train_features_df = train_features_df.drop('Split')\n",
    "test_features_df = test_features_df.drop('Split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad78dd-6087-4d77-b140-8b77df11396d",
   "metadata": {},
   "source": [
    "# Run Collaborative filtering at the Window level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef3b3f-905b-4548-b7fb-cfc3c788e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_windows = train_features_df.select('WindowID').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for window_id in unique_windows:\n",
    "    window_train_df = train_features_df.filter(train_features_df['WindowID'] == window_id)\n",
    "    window_test_df = test_features_df.filter(test_features_df['WindowID'] == window_id)\n",
    "    \"\"\"\n",
    "    Do Collaborative filtering here\n",
    "\n",
    "    Create indexers for user and item IDs\n",
    "    user_indexer = StringIndexer(inputCol=“userId”, outputCol=“userIndex”) \n",
    "    item_indexer = StringIndexer(inputCol=“jobId”, outputCol=“itemIndex”)\n",
    "\n",
    "    Create one-hot encoders for user and item indices\n",
    "    user_encoder = OneHotEncoder(inputCol=“userIndex”, outputCol=“userVec”) \n",
    "    item_encoder = OneHotEncoder(inputCol=“itemIndex”, outputCol=“itemVec”)\n",
    "\n",
    "    Create a vector assembler for user and item features\n",
    "    assembler = VectorAssembler(inputCols=[“userVec”, “user_feat1”, “user_feat2”, “user_feat3”, “itemVec”, “job_feat1”, “job_feat2”, “job_feat3”], outputCol=“features”)\n",
    "\n",
    "    Create an FM regressor\n",
    "    fm = FMRegressor(featuresCol=“features”, labelCol=“rating”)\n",
    "    TODO: Do we need a default rating of 1 here?\n",
    "\n",
    "    Create a pipeline to process the data and fit the model\n",
    "    from pyspark.ml import Pipeline pipeline = Pipeline(stages=[user_indexer, item_indexer, user_encoder, item_encoder, assembler, fm]) model = pipeline.fit(train_df)\n",
    "\n",
    "    Make predictions on the test data\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    Evaluate the model using RMSE metric\n",
    "    evaluator = RegressionEvaluator(metricName=“rmse”, labelCol=“rating”, predictionCol=“prediction”) rmse = evaluator.evaluate(predictions) print(f\"Root-mean-square error = {rmse}\")\n",
    "\n",
    "    # Generate recommendations for all users\n",
    "    userRecs = model.recommendForAllUsers(5)\n",
    "\n",
    "    # Generate top 5 recommendations for a specific user (replace 'userId' with the actual user ID)\n",
    "    user_id = 1\n",
    "    userRecs.filter(userRecs['UserID'] == user_id).select('recommendations').show()\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98316d8b-d997-4848-8f2a-3c4d69f67cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
